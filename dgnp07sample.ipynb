{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Didital Garage and Naviplus 主催\n",
    "\n",
    "# 第5回「Python で体験する機械学習」勉強会\n",
    "\n",
    "<center>\n",
    "<span style=\"color:Forestgreen\">浅川伸一</span> \n",
    "<span style=\"color:SpringGreen\">asakawa@ieee.org</span>\n",
    "</center>\n",
    "\n",
    "> 過去が現在に影響を与えるように、未来も現在に影響を与える。\n",
    "> ニーチェ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/env/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03508131]\n",
      " [ 0.95463069]\n",
      " [ 0.96195439]\n",
      " [ 0.05360758]]\n"
     ]
    }
   ],
   "source": [
    "# 入力データの定義\n",
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "\n",
    "# 教師データの定義\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# 乱数系列の初期化\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "# 結合係数行列の初期化\n",
    "Win  = 2 * np.random.random((3,4)) - 1\n",
    "Wout = 2 * np.random.random((4,1)) - 1\n",
    "\n",
    "# 学習ループ\n",
    "for i in  range(1000):\n",
    "    hidden = 1/(1+np.exp(-(np.dot(X,Win))))\n",
    "    out    = 1/(1+np.exp(-(np.dot(hidden,Wout))))\n",
    "    delta_out = ( y -  out ) * ( out * ( 1 - out ))\n",
    "    delta_hid = delta_out.dot(Wout.T) * ( hidden * (1 - hidden))\n",
    "    Wout += hidden.T.dot(delta_out)\n",
    "    Win  += X.T.dot(delta_hid)\n",
    "\n",
    "# 結果の表示\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](./assets/xor-graph.svg){!#fig:xor-graph style=\"width:24%\"}\n",
    "![image](./assets/xor.svg){!#fig:xor style=\"width:9%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# binary addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "\"\"\" https://www.youtube.com/watch?v=cdLUzrjnlr4\"\"\"\n",
    "\"\"\"i_rnn.py: test rnn code\"\"\"\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy    # for deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# シグモイド関数の定義\n",
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "# シグモイド関数の微分\n",
    "def d_sigmoid(x):\n",
    "    return x * (1.-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2 進数のデータを作成\n",
    "int2binary = {}\n",
    "binary_dim = 7\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "np.random.seed(seed=1)\n",
    "lr = 0.1             # 学習率 learning ratio\n",
    "MaxIter = 10000      # 繰り返しの最大数\n",
    "N_i = 2              # 入力層のニューロン数 (入力次元数)\n",
    "N_h = 8              # 中間層のニューロン数 (可変)\n",
    "N_o = 1              # 出力層のニューロン数     \n",
    "\n",
    "# 結合係数行列の初期化\n",
    "W_ih = 2 * np.random.random((N_i, N_h)) - 1.\n",
    "W_ho = 2 * np.random.random((N_h, N_o)) - 1.\n",
    "W_hh = 2 * np.random.random((N_h, N_h)) - 1.\n",
    "\n",
    "# 学習に用いる差分行列の定義\n",
    "dW_ih = np.zeros_like(W_ih)\n",
    "dW_ho = np.zeros_like(W_ho)\n",
    "dW_hh = np.zeros_like(W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下のセルは実行と関係なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[[0 0 0 0 0 0 1 1]\n",
      "  [0 0 0 0 0 1 0 0]]]\n",
      "Y= [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "for loop counter: 7\n"
     ]
    }
   ],
   "source": [
    "### 以下のループの意味\n",
    "a = int2binary[3]\n",
    "b = int2binary[4]\n",
    "X = np.array([[a, b]])\n",
    "print('X=',X)\n",
    "c = int2binary[7]\n",
    "# print(c)\n",
    "Y = np.array([c]).T\n",
    "print('Y=', Y)\n",
    "print('for loop counter:', binary_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下のセルは実行と関係なし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下のセルは実行と関係なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 5 4 3 2 1 0 \n"
     ]
    }
   ],
   "source": [
    "for position in range(binary_dim):\n",
    "    print(binary_dim - position - 1, end=' ')\n",
    "print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 具体例\n",
    "X = np.array([[a[3], b[3]]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration= 0\n",
      "Error: 3.733012\n",
      "Pred: [1 1 1 1 1 1 1 0]\n",
      "True: [0 0 1 0 0 0 1 1]\n",
      "33 + 2 = 254\n",
      "iteration= 1000\n",
      "Error: 3.150879\n",
      "Pred: [0 0 0 0 0 0 0 0]\n",
      "True: [0 1 0 1 0 0 1 1]\n",
      "51 + 32 = 0\n",
      "iteration= 2000\n",
      "Error: 3.021493\n",
      "Pred: [0 1 1 1 1 1 1 0]\n",
      "True: [0 1 0 1 1 1 1 1]\n",
      "47 + 48 = 126\n",
      "iteration= 3000\n",
      "Error: 3.043830\n",
      "Pred: [0 0 1 1 1 0 1 0]\n",
      "True: [0 1 0 1 1 0 1 1]\n",
      "61 + 30 = 58\n",
      "iteration= 4000\n",
      "Error: 3.055076\n",
      "Pred: [0 1 1 1 1 0 0 0]\n",
      "True: [0 1 0 0 0 0 0 1]\n",
      "12 + 53 = 120\n",
      "iteration= 5000\n",
      "Error: 2.757266\n",
      "Pred: [0 1 0 1 1 0 1 0]\n",
      "True: [0 1 0 1 0 0 1 1]\n",
      "46 + 37 = 90\n",
      "iteration= 6000\n",
      "Error: 2.688775\n",
      "Pred: [0 1 1 0 0 1 0 0]\n",
      "True: [0 1 0 0 1 1 0 0]\n",
      "28 + 48 = 100\n",
      "iteration= 7000\n",
      "Error: 2.060450\n",
      "Pred: [0 1 0 0 1 0 1 0]\n",
      "True: [0 1 0 0 1 0 1 0]\n",
      "34 + 40 = 74\n",
      "iteration= 8000\n",
      "Error: 3.562014\n",
      "Pred: [0 0 1 0 1 1 1 0]\n",
      "True: [0 1 0 0 0 0 0 0]\n",
      "23 + 41 = 46\n",
      "iteration= 9000\n",
      "Error: 1.881506\n",
      "Pred: [0 0 1 1 1 0 1 0]\n",
      "True: [0 0 1 1 1 0 1 0]\n",
      "20 + 38 = 58\n"
     ]
    }
   ],
   "source": [
    "for iter in range(MaxIter):\n",
    "    # 任意の数を生成して a に代入\n",
    "    a_int = np.random.randint(largest_number / 2)\n",
    "    a = int2binary[a_int]\n",
    "\n",
    "    # 任意の数を生成して b に代入\n",
    "    b_int = np.random.randint(largest_number / 2)\n",
    "    b = int2binary[b_int]\n",
    "\n",
    "    # a + b の答えを c に代入\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    total_err = 0\n",
    "    Output_deltas = list()\n",
    "    Hidden_values = list()\n",
    "    Hidden_values.append(np.zeros(N_h))\n",
    "\n",
    "    # forward prop\n",
    "    for position in range(binary_dim):\n",
    "        # a, b の上位桁から取り出して X に代入\n",
    "        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
    "        # y に c の上位桁から代入\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # 入力行列 X に W_ih をかけて，Hidden_values かける W_hh をたして\n",
    "        Hidden = sigmoid(np.dot(X, W_ih) + np.dot(Hidden_values[-1],W_hh))\n",
    "        Output = sigmoid(np.dot(Hidden, W_ho))\n",
    "\n",
    "        Delta = y - Output\n",
    "        Output_deltas.append((Delta) * d_sigmoid(Output))\n",
    "        total_err += np.abs(Delta[0])\n",
    "\n",
    "        ### prediction\n",
    "        d[binary_dim - position - 1] = np.round(Output[0][0])\n",
    "        Hidden_values.append(copy.deepcopy(Hidden))\n",
    "        #print('position=%d' % (position), end=' ')\n",
    "        #print(Hidden_values)\n",
    "\n",
    "    future_Hidden_delta = np.zeros(N_h)\n",
    "\n",
    "    # back prop\n",
    "    for position in range(binary_dim):\n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        Hidden       = Hidden_values[-position - 1]\n",
    "        prev_Hidden  = Hidden_values[-position - 2]\n",
    "\n",
    "        Output_delta = Output_deltas[-position - 1]\n",
    "        Hidden_delta = (future_Hidden_delta.dot(W_hh.T) + Output_delta.dot(W_ho.T)) * d_sigmoid(Hidden)\n",
    "\n",
    "        dW_ho += np.atleast_2d(     Hidden).T.dot(Output_delta)\n",
    "        dW_hh += np.atleast_2d(prev_Hidden).T.dot(Hidden_delta)\n",
    "        dW_ih += X.T.dot(Hidden_delta)\n",
    "        future_Hiddne_delta = Hidden_delta\n",
    "\n",
    "    W_ho += lr * dW_ho\n",
    "    W_hh += lr * dW_hh\n",
    "    W_ih += lr * dW_ih\n",
    "\n",
    "    dW_ho *= 0\n",
    "    dW_hh *= 0\n",
    "    dW_ih *= 0\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        print(\"iteration=\",iter)\n",
    "        print(\"Error: %f\" % (total_err))\n",
    "        print(\"Pred: %s\" % (str(d)))\n",
    "        print(\"True: %s\" % (str(c)))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "# sequential mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "'''\n",
    "A Reccurent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.812274, Training Accuracy= 0.35156\n",
      "Iter 2560, Minibatch Loss= 1.503676, Training Accuracy= 0.56250\n",
      "Iter 3840, Minibatch Loss= 1.173299, Training Accuracy= 0.58594\n",
      "Iter 5120, Minibatch Loss= 0.976452, Training Accuracy= 0.64844\n",
      "Iter 6400, Minibatch Loss= 0.719587, Training Accuracy= 0.76562\n",
      "Iter 7680, Minibatch Loss= 1.163240, Training Accuracy= 0.59375\n",
      "Iter 8960, Minibatch Loss= 0.764033, Training Accuracy= 0.75000\n",
      "Iter 10240, Minibatch Loss= 0.645423, Training Accuracy= 0.79688\n",
      "Iter 11520, Minibatch Loss= 0.369868, Training Accuracy= 0.94531\n",
      "Iter 12800, Minibatch Loss= 0.572701, Training Accuracy= 0.79688\n",
      "Iter 14080, Minibatch Loss= 0.547925, Training Accuracy= 0.83594\n",
      "Iter 15360, Minibatch Loss= 0.387878, Training Accuracy= 0.89062\n",
      "Iter 16640, Minibatch Loss= 0.358034, Training Accuracy= 0.93750\n",
      "Iter 17920, Minibatch Loss= 0.328301, Training Accuracy= 0.90625\n",
      "Iter 19200, Minibatch Loss= 0.340854, Training Accuracy= 0.86719\n",
      "Iter 20480, Minibatch Loss= 0.173010, Training Accuracy= 0.93750\n",
      "Iter 21760, Minibatch Loss= 0.477902, Training Accuracy= 0.84375\n",
      "Iter 23040, Minibatch Loss= 0.147933, Training Accuracy= 0.95312\n",
      "Iter 24320, Minibatch Loss= 0.344937, Training Accuracy= 0.92188\n",
      "Iter 25600, Minibatch Loss= 0.368267, Training Accuracy= 0.86719\n",
      "Iter 26880, Minibatch Loss= 0.232065, Training Accuracy= 0.89844\n",
      "Iter 28160, Minibatch Loss= 0.211251, Training Accuracy= 0.92969\n",
      "Iter 29440, Minibatch Loss= 0.293302, Training Accuracy= 0.88281\n",
      "Iter 30720, Minibatch Loss= 0.300081, Training Accuracy= 0.89844\n",
      "Iter 32000, Minibatch Loss= 0.207910, Training Accuracy= 0.91406\n",
      "Iter 33280, Minibatch Loss= 0.329312, Training Accuracy= 0.86719\n",
      "Iter 34560, Minibatch Loss= 0.318098, Training Accuracy= 0.90625\n",
      "Iter 35840, Minibatch Loss= 0.253779, Training Accuracy= 0.90625\n",
      "Iter 37120, Minibatch Loss= 0.289528, Training Accuracy= 0.92188\n",
      "Iter 38400, Minibatch Loss= 0.111428, Training Accuracy= 0.96875\n",
      "Iter 39680, Minibatch Loss= 0.148740, Training Accuracy= 0.96094\n",
      "Iter 40960, Minibatch Loss= 0.329879, Training Accuracy= 0.89844\n",
      "Iter 42240, Minibatch Loss= 0.124617, Training Accuracy= 0.96094\n",
      "Iter 43520, Minibatch Loss= 0.192036, Training Accuracy= 0.93750\n",
      "Iter 44800, Minibatch Loss= 0.190553, Training Accuracy= 0.94531\n",
      "Iter 46080, Minibatch Loss= 0.153427, Training Accuracy= 0.94531\n",
      "Iter 47360, Minibatch Loss= 0.269518, Training Accuracy= 0.91406\n",
      "Iter 48640, Minibatch Loss= 0.259179, Training Accuracy= 0.92969\n",
      "Iter 49920, Minibatch Loss= 0.241605, Training Accuracy= 0.91406\n",
      "Iter 51200, Minibatch Loss= 0.108841, Training Accuracy= 0.95312\n",
      "Iter 52480, Minibatch Loss= 0.200141, Training Accuracy= 0.94531\n",
      "Iter 53760, Minibatch Loss= 0.055500, Training Accuracy= 0.97656\n",
      "Iter 55040, Minibatch Loss= 0.094812, Training Accuracy= 0.96875\n",
      "Iter 56320, Minibatch Loss= 0.209431, Training Accuracy= 0.90625\n",
      "Iter 57600, Minibatch Loss= 0.137725, Training Accuracy= 0.96094\n",
      "Iter 58880, Minibatch Loss= 0.106962, Training Accuracy= 0.97656\n",
      "Iter 60160, Minibatch Loss= 0.142860, Training Accuracy= 0.94531\n",
      "Iter 61440, Minibatch Loss= 0.070273, Training Accuracy= 0.98438\n",
      "Iter 62720, Minibatch Loss= 0.191168, Training Accuracy= 0.94531\n",
      "Iter 64000, Minibatch Loss= 0.221378, Training Accuracy= 0.92969\n",
      "Iter 65280, Minibatch Loss= 0.191530, Training Accuracy= 0.95312\n",
      "Iter 66560, Minibatch Loss= 0.190562, Training Accuracy= 0.95312\n",
      "Iter 67840, Minibatch Loss= 0.085384, Training Accuracy= 0.97656\n",
      "Iter 69120, Minibatch Loss= 0.113092, Training Accuracy= 0.96875\n",
      "Iter 70400, Minibatch Loss= 0.150146, Training Accuracy= 0.95312\n",
      "Iter 71680, Minibatch Loss= 0.155853, Training Accuracy= 0.95312\n",
      "Iter 72960, Minibatch Loss= 0.078432, Training Accuracy= 0.98438\n",
      "Iter 74240, Minibatch Loss= 0.071382, Training Accuracy= 0.96875\n",
      "Iter 75520, Minibatch Loss= 0.106345, Training Accuracy= 0.96094\n",
      "Iter 76800, Minibatch Loss= 0.094373, Training Accuracy= 0.98438\n",
      "Iter 78080, Minibatch Loss= 0.098409, Training Accuracy= 0.96875\n",
      "Iter 79360, Minibatch Loss= 0.158386, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 0.106841, Training Accuracy= 0.96875\n",
      "Iter 81920, Minibatch Loss= 0.107378, Training Accuracy= 0.96094\n",
      "Iter 83200, Minibatch Loss= 0.145097, Training Accuracy= 0.93750\n",
      "Iter 84480, Minibatch Loss= 0.091690, Training Accuracy= 0.96094\n",
      "Iter 85760, Minibatch Loss= 0.188187, Training Accuracy= 0.93750\n",
      "Iter 87040, Minibatch Loss= 0.075445, Training Accuracy= 0.99219\n",
      "Iter 88320, Minibatch Loss= 0.071755, Training Accuracy= 0.98438\n",
      "Iter 89600, Minibatch Loss= 0.099668, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 0.149910, Training Accuracy= 0.95312\n",
      "Iter 92160, Minibatch Loss= 0.089376, Training Accuracy= 0.96094\n",
      "Iter 93440, Minibatch Loss= 0.107499, Training Accuracy= 0.96875\n",
      "Iter 94720, Minibatch Loss= 0.084978, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 0.049506, Training Accuracy= 0.99219\n",
      "Iter 97280, Minibatch Loss= 0.085726, Training Accuracy= 0.97656\n",
      "Iter 98560, Minibatch Loss= 0.069938, Training Accuracy= 0.97656\n",
      "Iter 99840, Minibatch Loss= 0.124236, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976562\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
